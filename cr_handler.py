"""
Generated by Cursor AI Assistant
https://cursor.sh

Custom Resource handler for managing OpenStack Tempest test CRs.
"""

import logging
import re
import subprocess
import json
import time
from datetime import datetime
from typing import Dict, Optional, Tuple
import threading
import yaml

logger = logging.getLogger(__name__)


class CRHandler:
    """Handles Custom Resource operations for Tempest tests."""
    
    def __init__(self, namespace: str, timeout: int = 3600):
        """
        Initialize the CR handler.
        
        Args:
            namespace: Kubernetes namespace for CRs
            timeout: Timeout for CR completion in seconds
        """
        self.namespace = namespace
        self.timeout = timeout
        self.active_crs = {}  # Track active CRs
        
    def apply_cr(self, cr_file: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        Apply a Custom Resource file.
        
        Args:
            cr_file: Path to the CR YAML file
            
        Returns:
            Tuple of (success, cr_name, error_message)
        """
        try:
            # Read CR file to extract name
            with open(cr_file, 'r') as f:
                cr_data = yaml.safe_load(f)
            
            cr_name = cr_data.get("metadata", {}).get("name", "unknown")
            cr_kind = cr_data.get("kind", "unknown")
            
            logger.info(f"Applying CR from {cr_file} (name: {cr_name}, kind: {cr_kind})")
            
            cmd = ["oc", "apply", "-f", cr_file, "-n", self.namespace]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            logger.info(f"Successfully applied CR: {result.stdout.strip()}")
            
            # Track the CR
            self.active_crs[cr_name] = {
                "file": cr_file,
                "kind": cr_kind,
                "start_time": datetime.now(),
                "status": "Running"
            }
            
            return True, cr_name, None
            
        except FileNotFoundError:
            error_msg = f"CR file not found: {cr_file}"
            logger.error(error_msg)
            return False, None, error_msg
        except subprocess.CalledProcessError as e:
            error_msg = f"Failed to apply CR {cr_file}: {e.stderr}"
            logger.error(error_msg)
            return False, None, error_msg
        except Exception as e:
            error_msg = f"Unexpected error applying CR {cr_file}: {str(e)}"
            logger.error(error_msg)
            return False, None, error_msg
    
    def wait_for_pod_to_start(self, cr_name: str, timeout: int = 180, 
                              shutdown_flag: Optional[threading.Event] = None) -> Tuple[bool, str]:
        """
        Wait for a pod associated with the CR to start running.
        
        Args:
            cr_name: Name of the CR
            timeout: Timeout in seconds to wait for pod to start (default: 180s = 3min)
            shutdown_flag: Optional shutdown event to allow graceful interruption
            
        Returns:
            Tuple of (success, message)
        """
        start_time = time.time()
        poll_interval = 5  # Check every 5 seconds
        
        logger.info(f"Waiting for pod to start for CR: {cr_name} (timeout: {timeout}s)")
        
        while time.time() - start_time < timeout:
            # Check for shutdown request
            if shutdown_flag and shutdown_flag.is_set():
                msg = f"Pod startup check interrupted by shutdown"
                logger.info(msg)
                return False, msg
            
            try:
                # Get pods matching the CR name pattern
                cmd = ["oc", "get", "pods", "-n", self.namespace, "-o", "json"]
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                pods_data = json.loads(result.stdout)
                
                # Look for pods that match the CR name
                for pod in pods_data.get("items", []):
                    pod_name = pod["metadata"]["name"]
                    
                    if cr_name in pod_name:
                        phase = pod.get("status", {}).get("phase", "Unknown")
                        
                        # Check if pod is Running or Succeeded
                        if phase in ["Running", "Succeeded"]:
                            msg = f"Pod {pod_name} is {phase}"
                            logger.info(msg)
                            return True, msg
                        
                        # Check if pod is Failed
                        elif phase == "Failed":
                            container_statuses = pod.get("status", {}).get("containerStatuses", [])
                            reason = "Unknown"
                            if container_statuses:
                                reason = container_statuses[0].get("state", {}).get("terminated", {}).get("reason", "Unknown")
                            
                            msg = f"Pod {pod_name} failed to start (Phase: {phase}, Reason: {reason})"
                            logger.error(msg)
                            return False, msg
                        
                        # Pod exists but not running yet
                        logger.debug(f"Pod {pod_name} is in phase: {phase}")
                
            except subprocess.CalledProcessError as e:
                logger.debug(f"Failed to get pods: {e.stderr}")
            except json.JSONDecodeError as e:
                logger.debug(f"Failed to parse pods JSON: {e}")
            
            # Wait before next check
            if shutdown_flag:
                if shutdown_flag.wait(timeout=poll_interval):
                    msg = f"Pod startup check interrupted by shutdown"
                    logger.info(msg)
                    return False, msg
            else:
                time.sleep(poll_interval)
        
        # Timeout reached
        msg = f"Timeout waiting for pod to start for CR: {cr_name} (waited {timeout}s)"
        logger.error(msg)
        return False, msg
    
    def get_cr_status(self, cr_name: str, kind: str = None) -> Dict:
        """
        Get the status of a Custom Resource.
        
        Args:
            cr_name: Name of the CR
            kind: Kind of the CR (optional, will try to detect)
            
        Returns:
            Dictionary with CR status information
        """
        try:
            # If kind is not provided, try to get it from active_crs
            if kind is None and cr_name in self.active_crs:
                kind = self.active_crs[cr_name]["kind"]
            
            # Try different common resource types if kind is unknown
            resource_types = [kind] if kind else ["tempest", "tempestrun", "test"]
            
            for res_type in resource_types:
                if not res_type:
                    continue
                    
                try:
                    cmd = ["oc", "get", res_type, cr_name, "-n", self.namespace, "-o", "json"]
                    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                    cr_data = json.loads(result.stdout)
                    
                    status = cr_data.get("status", {})
                    phase = status.get("phase", status.get("state", "Unknown"))
                    
                    # Also check pod status - if pod is in Error, test is done
                    pod_completed = self._check_pod_completion(cr_name)
                    
                    # Consider completed if either CR says so OR pod is in terminal state
                    is_completed = self._is_completed(status) or pod_completed
                    
                    return {
                        "phase": phase,
                        "completed": is_completed,
                        "succeeded": self._is_succeeded(status) and not pod_completed,
                        "message": status.get("message", ""),
                        "raw_status": status
                    }
                except subprocess.CalledProcessError:
                    continue
            
            logger.warning(f"Could not find CR {cr_name} with any known resource type")
            return {
                "phase": "NotFound",
                "completed": False,
                "succeeded": False,
                "message": "CR not found",
                "raw_status": {}
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse CR status for {cr_name}: {e}")
            return {
                "phase": "Error",
                "completed": False,
                "succeeded": False,
                "message": "Failed to parse status",
                "raw_status": {}
            }
    
    def _check_pod_completion(self, cr_name: str) -> bool:
        """
        Check if pods associated with the CR are in terminal state and have completed tests.
        
        Args:
            cr_name: Name of the CR
            
        Returns:
            True if pod is in terminal state with completed tests, False otherwise
        """
        try:
            cmd = ["oc", "get", "pods", "-n", self.namespace, "-o", "json"]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            pods_data = json.loads(result.stdout)
            
            # Look for pods matching the CR name
            for pod in pods_data.get("items", []):
                pod_name = pod["metadata"]["name"]
                
                if cr_name in pod_name:
                    phase = pod.get("status", {}).get("phase", "Unknown")
                    logger.debug(f"Found pod {pod_name} for CR {cr_name} with phase: {phase}")
                    
                    # Terminal states: Error, Failed, Succeeded
                    if phase in ["Error", "Failed", "Succeeded"]:
                        # For Error/Failed pods, check logs to see if tests actually completed
                        if phase in ["Error", "Failed"]:
                            if self._check_test_completion_in_logs(pod_name):
                                logger.info(f"Pod {pod_name} is in {phase} state but tests completed - marking CR as completed")
                                return True
                            else:
                                logger.debug(f"Pod {pod_name} is in {phase} state but no test completion found in logs")
                                return False
                        else:
                            # Succeeded - definitely completed
                            logger.info(f"Pod {pod_name} is in terminal state: {phase} - marking CR as completed")
                            return True
            
            return False
            
        except Exception as e:
            logger.debug(f"Failed to check pod completion for {cr_name}: {e}")
            return False
    
    def _check_test_completion_in_logs(self, pod_name: str) -> bool:
        """
        Check pod logs for test completion indicators (Totals, Ran: X tests).
        
        Args:
            pod_name: Name of the pod to check
            
        Returns:
            True if test completion found in logs, False otherwise
        """
        try:
            # Get pod logs
            cmd = ["oc", "logs", pod_name, "-n", self.namespace, "--tail=100"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                logs = result.stdout
                
                # Check for test completion indicators
                # Look for "Totals" and "Ran: X tests" which indicate test run completed
                if "Totals" in logs and "Ran:" in logs and "tests" in logs:
                    logger.info(f"Found test completion in logs for pod {pod_name}")
                    
                    # Extract test summary for logging
                    for line in logs.split('\n'):
                        if "Ran:" in line or "Passed:" in line or "Failed:" in line:
                            logger.info(f"  {line.strip()}")
                    
                    return True
                else:
                    logger.debug(f"No test completion indicators found in logs for {pod_name}")
                    return False
            else:
                logger.debug(f"Failed to get logs for {pod_name}: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.warning(f"Timeout getting logs for {pod_name}")
            return False
        except Exception as e:
            logger.debug(f"Error checking logs for {pod_name}: {e}")
            return False
    
    def extract_failed_tests(self, cr_name: str, iteration: int = 0) -> list:
        """
        Extract failed test details from pod logs.
        
        Args:
            cr_name: Name of the CR
            iteration: Current iteration number
            
        Returns:
            List of dictionaries containing failed test information
        """
        failed_tests = []
        
        try:
            # Find pods for this CR
            cmd = ["oc", "get", "pods", "-n", self.namespace, "-o", "json"]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            pods_data = json.loads(result.stdout)
            
            for pod in pods_data.get("items", []):
                pod_name = pod["metadata"]["name"]
                
                if cr_name in pod_name:
                    # Get full pod logs
                    log_cmd = ["oc", "logs", pod_name, "-n", self.namespace]
                    log_result = subprocess.run(log_cmd, capture_output=True, text=True, timeout=60)
                    
                    if log_result.returncode == 0:
                        logs = log_result.stdout
                        
                        # Parse for FAILED tests
                        # Format: {3} test.class.name [time] ... FAILED
                        failed_pattern = r'\{(\d+)\}\s+([^\[]+)\s+\[([^\]]+)\]\s+\.\.\.\s+FAILED'
                        
                        for line in logs.split('\n'):
                            match = re.search(failed_pattern, line)
                            if match:
                                test_number = match.group(1)
                                test_name = match.group(2).strip()
                                duration = match.group(3).strip()
                                raw_line = line.strip()  # Preserve original log line
                                
                                failed_test = {
                                    'timestamp': datetime.now().isoformat(),
                                    'iteration': iteration,
                                    'cr_name': cr_name,
                                    'pod_name': pod_name,
                                    'test_number': test_number,
                                    'test_name': test_name,
                                    'duration': duration,
                                    'logged_line': raw_line
                                }
                                
                                failed_tests.append(failed_test)
                                logger.info(f"Found failed test: {{{test_number}}} {test_name} [{duration}] ... FAILED")
                    
                    # Only check the first matching pod
                    break
            
            return failed_tests
            
        except subprocess.TimeoutExpired:
            logger.warning(f"Timeout getting logs for CR {cr_name}")
            return []
        except Exception as e:
            logger.error(f"Error extracting failed tests for {cr_name}: {e}")
            return []
    
    def extract_test_execution_times(self, cr_name: str, iteration: int = 1) -> List[Dict]:
        """
        Extract execution times for all tests from pod logs.
        
        Args:
            cr_name: Name of the Custom Resource
            iteration: Current iteration number
            
        Returns:
            List of dictionaries containing test execution details
        """
        test_executions = []
        
        try:
            # Get pods associated with this CR
            cmd = ["oc", "get", "pods", "-n", self.namespace, "-o", "json"]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            pods_data = json.loads(result.stdout)
            
            # Find pod associated with this CR
            for pod in pods_data.get("items", []):
                pod_name = pod["metadata"]["name"]
                
                if cr_name in pod_name:
                    # Get logs
                    log_cmd = ["oc", "logs", pod_name, "-n", self.namespace]
                    log_result = subprocess.run(log_cmd, capture_output=True, text=True, check=False)
                    
                    if log_result.returncode == 0:
                        logs = log_result.stdout
                        
                        # Parse for all test executions (PASSED, FAILED, SKIPPED)
                        # Format: {3} test.class.name [time] ... STATUS
                        test_pattern = r'\{(\d+)\}\s+([^\[]+)\s+\[([^\]]+)\]\s+\.\.\.\s+(PASSED|FAILED|SKIPPED|OK)'
                        
                        for line in logs.split('\n'):
                            match = re.search(test_pattern, line)
                            if match:
                                test_number = match.group(1)
                                test_name = match.group(2).strip()
                                duration_str = match.group(3).strip()
                                status = match.group(4)
                                
                                # Convert duration to seconds (remove 's' if present)
                                duration_seconds = float(duration_str.rstrip('s'))
                                
                                test_execution = {
                                    'timestamp': datetime.now().isoformat(),
                                    'iteration': iteration,
                                    'cr_name': cr_name,
                                    'pod_name': pod_name,
                                    'test_number': test_number,
                                    'test_name': test_name,
                                    'duration_seconds': duration_seconds,
                                    'status': status
                                }
                                
                                test_executions.append(test_execution)
                        
                        if test_executions:
                            logger.info(f"Extracted {len(test_executions)} test execution times from {pod_name}")
                    
                    # Only check the first matching pod
                    break
            
            return test_executions
        
        except Exception as e:
            logger.error(f"Error extracting test execution times from logs for {cr_name}: {e}")
            return []
    
    def wait_for_completion(self, cr_name: str, poll_interval: int = 30, 
                           shutdown_flag: Optional[threading.Event] = None) -> Tuple[bool, str]:
        """
        Wait for a CR to complete.
        
        Args:
            cr_name: Name of the CR
            poll_interval: Polling interval in seconds
            shutdown_flag: Optional shutdown event to allow graceful interruption
            
        Returns:
            Tuple of (success, message)
        """
        start_time = time.time()
        
        while time.time() - start_time < self.timeout:
            # Check for shutdown request
            if shutdown_flag and shutdown_flag.is_set():
                msg = f"CR {cr_name} monitoring interrupted by shutdown"
                logger.info(msg)
                return False, msg
            
            status = self.get_cr_status(cr_name)
            
            if status["completed"]:
                if status["succeeded"]:
                    msg = f"CR {cr_name} completed successfully"
                    logger.info(msg)
                    return True, msg
                else:
                    msg = f"CR {cr_name} completed with failure (phase: {status['phase']}): {status['message']}"
                    logger.warning(msg)
                    return False, msg
            
            logger.info(f"CR {cr_name} still running (phase: {status['phase']}, completed: {status['completed']})")
            
            # Use shutdown_flag.wait() if available to allow interruption
            if shutdown_flag:
                if shutdown_flag.wait(timeout=poll_interval):
                    # Shutdown was signaled during wait
                    msg = f"CR {cr_name} monitoring interrupted by shutdown"
                    logger.info(msg)
                    return False, msg
            else:
                time.sleep(poll_interval)
        
        msg = f"CR {cr_name} timed out after {self.timeout} seconds"
        logger.error(msg)
        return False, msg
    
    def _extract_test_counts_from_logs(self, cr_name: str) -> Dict:
        """
        Extract test counts from pod logs.
        
        Args:
            cr_name: Name of the CR
            
        Returns:
            Dictionary with test counts or None if not found
        """
        try:
            # Get pods associated with this CR
            cmd = ["oc", "get", "pods", "-n", self.namespace, "-o", "json"]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            pods_data = json.loads(result.stdout)
            
            # Find pod associated with this CR
            for pod in pods_data.get("items", []):
                pod_name = pod["metadata"]["name"]
                
                if cr_name in pod_name:
                    # Get logs
                    log_cmd = ["oc", "logs", pod_name, "-n", self.namespace]
                    log_result = subprocess.run(log_cmd, capture_output=True, text=True, check=False)
                    
                    if log_result.returncode == 0:
                        logs = log_result.stdout
                        
                        # Parse test counts from logs
                        # Format: 
                        # Ran: 6 tests in 327.7494 sec.
                        #  - Passed: 5
                        #  - Skipped: 0
                        #  - Failed: 1
                        
                        passed_match = re.search(r'- Passed:\s+(\d+)', logs)
                        failed_match = re.search(r'- Failed:\s+(\d+)', logs)
                        skipped_match = re.search(r'- Skipped:\s+(\d+)', logs)
                        
                        if passed_match or failed_match or skipped_match:
                            counts = {
                                "passed": int(passed_match.group(1)) if passed_match else 0,
                                "failed": int(failed_match.group(1)) if failed_match else 0,
                                "skipped": int(skipped_match.group(1)) if skipped_match else 0
                            }
                            logger.debug(f"Extracted test counts from pod logs for {cr_name}: {counts}")
                            return counts
                    
                    # Only check the first matching pod
                    break
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting test counts from logs for {cr_name}: {e}")
            return None
    
    def check_test_results(self, cr_name: str) -> Dict:
        """
        Check the test results from a completed CR.
        
        Args:
            cr_name: Name of the CR
            
        Returns:
            Dictionary with test results
        """
        status = self.get_cr_status(cr_name)
        
        # Try to extract test results from status
        raw_status = status.get("raw_status", {})
        
        # First try to get from CR status
        tests_passed = raw_status.get("testsPassed", 0)
        tests_failed = raw_status.get("testsFailed", 0)
        tests_skipped = raw_status.get("testsSkipped", 0)
        
        # If not found, try to extract from pod logs
        if tests_passed == 0 and tests_failed == 0 and tests_skipped == 0:
            test_counts = self._extract_test_counts_from_logs(cr_name)
            if test_counts:
                tests_passed = test_counts.get("passed", 0)
                tests_failed = test_counts.get("failed", 0)
                tests_skipped = test_counts.get("skipped", 0)
        
        results = {
            "cr_name": cr_name,
            "passed": status["succeeded"],
            "phase": status["phase"],
            "message": status["message"],
            "tests_passed": tests_passed,
            "tests_failed": tests_failed,
            "tests_skipped": tests_skipped,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.debug(f"Test results for {cr_name}: Passed={tests_passed}, Failed={tests_failed}, Skipped={tests_skipped}")
        
        return results
    
    def delete_cr(self, cr_name: str) -> bool:
        """
        Delete a Custom Resource using its original file.
        
        Args:
            cr_name: Name of the CR
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get the CR file from active tracking
            if cr_name in self.active_crs:
                cr_file = self.active_crs[cr_name]["file"]
                
                # Delete using the original file (proper way - cleans up all resources)
                cmd = ["oc", "delete", "-f", cr_file, "-n", self.namespace, "--ignore-not-found"]
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                logger.info(f"Deleted CR {cr_name} using file {cr_file}: {result.stdout.strip()}")
                
                # Remove from active tracking
                del self.active_crs[cr_name]
                return True
            else:
                # Fallback: try to delete by resource type if we don't have the file
                logger.warning(f"CR {cr_name} not in active tracking, trying fallback deletion")
                resource_types = ["tempest", "tempestrun", "test"]
                
                for res_type in resource_types:
                    try:
                        cmd = ["oc", "delete", res_type, cr_name, "-n", self.namespace, "--ignore-not-found"]
                        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                        logger.info(f"Deleted CR {cr_name}: {result.stdout.strip()}")
                        return True
                    except subprocess.CalledProcessError:
                        continue
                
                logger.warning(f"Could not delete CR {cr_name}")
                return False
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to delete CR {cr_name}: {e.stderr}")
            return False
        except Exception as e:
            logger.error(f"Failed to delete CR {cr_name}: {e}")
            return False
    
    def _is_completed(self, status: Dict) -> bool:
        """Check if CR is completed based on status."""
        phase = status.get("phase", status.get("state", "")).lower()
        return phase in ["succeeded", "failed", "complete", "completed", "error"]
    
    def _is_succeeded(self, status: Dict) -> bool:
        """Check if CR succeeded based on status."""
        phase = status.get("phase", status.get("state", "")).lower()
        return phase in ["succeeded", "complete", "completed"] and status.get("error", "") == ""

