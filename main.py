#!/usr/bin/env python3
"""
Generated by Cursor AI Assistant
https://cursor.sh

OpenStack Tempest Test Runner with Pod Monitoring

This tool runs OpenStack Tempest tests via OpenShift Custom Resources
in parallel and monitors pod metrics (CPU, RAM, status) throughout the test runs.
"""

import argparse
import logging
import os
import signal
import socket
import sys
import threading
import time
import yaml
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple

from pod_monitor import PodMonitor
from cr_handler import CRHandler
from csv_exporter import CSVExporter
from api_monitor import APIMonitor
from error_collector import ErrorCollector


# Global flag for graceful shutdown
shutdown_flag = threading.Event()


def setup_logging(log_level: str, log_file: str):
    """Setup logging configuration."""
    numeric_level = getattr(logging, log_level.upper(), logging.INFO)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(numeric_level)
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(console_formatter)
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(numeric_level)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)


def load_config(config_file: str) -> Dict:
    """Load configuration from YAML file."""
    try:
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        logging.error(f"Configuration file not found: {config_file}")
        sys.exit(1)
    except yaml.YAMLError as e:
        logging.error(f"Error parsing configuration file: {e}")
        sys.exit(1)


def signal_handler(signum, frame):
    """Handle interrupt signals for graceful shutdown."""
    logging.info("Received interrupt signal. Initiating graceful shutdown...")
    shutdown_flag.set()


def print_error_bold_red(message: str):
    """Print error message in bold red text."""
    # ANSI color codes: \033[1;31m = bold red, \033[0m = reset
    RED_BOLD = "\033[1;31m"
    RESET = "\033[0m"
    print(f"{RED_BOLD}{message}{RESET}")
    logging.error(message)


def run_cr_with_monitoring(cr_file: str, cr_handler: CRHandler, iteration: int) -> Dict:
    """
    Run a single CR and monitor until completion.
    
    Args:
        cr_file: Path to CR file
        cr_handler: CRHandler instance
        iteration: Current iteration number
        
    Returns:
        Dictionary with test results
    """
    logger = logging.getLogger(__name__)
    
    # Apply the CR
    success, cr_name, error = cr_handler.apply_cr(cr_file)
    
    if not success:
        logger.error(f"Failed to apply CR {cr_file}: {error}")
        return {
            "cr_name": cr_file,
            "passed": False,
            "phase": "ApplyFailed",
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "message": error,
            "timestamp": datetime.now().isoformat(),
            "iteration": iteration
        }
    
    logger.info(f"[Iteration {iteration}] Started CR: {cr_name} from {cr_file}")
    
    # Wait for pod to actually start (fail fast if pod doesn't start)
    pod_started, pod_message = cr_handler.wait_for_pod_to_start(
        cr_name, 
        timeout=180,  # 3 minutes to start
        shutdown_flag=shutdown_flag
    )
    
    if not pod_started:
        error_msg = f"ERROR: Test pod failed to start for CR {cr_name}!"
        detail_msg = f"Details: {pod_message}"
        explanation = "This indicates a problem with the test setup or environment."
        action = "Stopping tool execution immediately."
        
        print_error_bold_red("\n" + "="*60)
        print_error_bold_red(error_msg)
        print_error_bold_red(detail_msg)
        print_error_bold_red(explanation)
        print_error_bold_red(action)
        print_error_bold_red("="*60 + "\n")
        
        # Clean up the failed CR
        logger.info(f"Cleaning up failed CR: {cr_name}")
        cr_handler.delete_cr(cr_name)
        
        # Signal shutdown to stop the tool
        shutdown_flag.set()
        
        return {
            "cr_name": cr_file,
            "passed": False,
            "phase": "PodStartupFailed",
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "message": f"Pod failed to start: {pod_message}",
            "timestamp": datetime.now().isoformat(),
            "iteration": iteration
        }
    
    # Wait for completion (non-blocking, returns after timeout or completion)
    # Pass shutdown_flag to allow graceful interruption
    success, message = cr_handler.wait_for_completion(cr_name, poll_interval=30, shutdown_flag=shutdown_flag)
    
    # Check results
    results = cr_handler.check_test_results(cr_name)
    results["iteration"] = iteration
    
    if not success:
        logger.error(f"[Iteration {iteration}] CR {cr_name} failed: {message}")
    else:
        logger.info(f"[Iteration {iteration}] CR {cr_name} completed successfully")
    
    # Extract failed tests from pod logs before cleanup
    logger.info(f"[Iteration {iteration}] Extracting failed tests for CR: {cr_name}")
    failed_tests = cr_handler.extract_failed_tests(cr_name, iteration)
    
    # Extract test execution times from pod logs before cleanup
    logger.info(f"[Iteration {iteration}] Extracting test execution times for CR: {cr_name}")
    test_execution_times = cr_handler.extract_test_execution_times(cr_name, iteration)
    
    # Store failed tests and execution times in results for export
    results["failed_tests"] = failed_tests
    results["test_execution_times"] = test_execution_times
    
    # Clean up CR after completion (needed for next iteration)
    # Test-operator leaves pods in ERROR status when complete
    logger.info(f"[Iteration {iteration}] Cleaning up CR: {cr_name}")
    if cr_handler.delete_cr(cr_name):
        logger.info(f"[Iteration {iteration}] Successfully deleted CR: {cr_name}")
    else:
        logger.warning(f"[Iteration {iteration}] Failed to delete CR: {cr_name}, may need manual cleanup")
    
    return results


def monitor_pods_loop(pod_monitor: PodMonitor, csv_exporter: CSVExporter, interval: int):
    """
    Continuously monitor pods in a separate thread.
    
    Args:
        pod_monitor: PodMonitor instance
        csv_exporter: CSVExporter instance
        interval: Monitoring interval in seconds
    """
    logger = logging.getLogger(__name__)
    logger.info("Started pod monitoring loop")
    
    while not shutdown_flag.is_set():
        try:
            # Collect metrics
            metrics = pod_monitor.collect_metrics()
            
            if metrics:
                logger.debug(f"Collected metrics for {len(metrics)} pods")
                # Export to CSV
                csv_exporter.export_metrics(metrics)
            
            # Wait for next interval
            shutdown_flag.wait(timeout=interval)
            
        except Exception as e:
            logger.error(f"Error in monitoring loop: {e}")
            time.sleep(interval)
    
    logger.info("Pod monitoring loop stopped")


def run_tests_in_loop(cr_files: List[str], cr_handler: CRHandler, csv_exporter: CSVExporter, 
                      end_time: datetime, max_parallel: int = 2) -> Tuple[List[Dict], datetime]:
    """
    Run CR tests in a loop until end_time.
    
    Args:
        cr_files: List of CR files to run
        cr_handler: CRHandler instance
        csv_exporter: CSVExporter instance
        end_time: Time to stop running tests
        max_parallel: Maximum parallel CR executions
        
    Returns:
        Tuple of (all_test_results, last_iteration_start_time)
        - all_test_results: List of all test results from all iterations
        - last_iteration_start_time: Start time of the last iteration (used for error log collection)
    """
    logger = logging.getLogger(__name__)
    all_results = []
    iteration = 1
    last_iteration_start_time = datetime.now()  # Track start time of last iteration for error collection
    
    logger.info(f"Starting test loop with {len(cr_files)} CR files")
    logger.info(f"Will run until: {end_time}")
    
    while datetime.now() < end_time and not shutdown_flag.is_set():
        # Track the start time of this iteration
        iteration_start_time = datetime.now()
        last_iteration_start_time = iteration_start_time
        
        logger.info(f"\n{'='*60}")
        logger.info(f"Starting iteration {iteration}")
        logger.info(f"{'='*60}\n")
        
        # Run CRs in parallel
        with ThreadPoolExecutor(max_workers=max_parallel) as executor:
            futures = {}
            
            for cr_file in cr_files:
                if shutdown_flag.is_set():
                    break
                    
                future = executor.submit(run_cr_with_monitoring, cr_file, cr_handler, iteration)
                futures[future] = cr_file
            
            # Collect results as they complete
            for future in as_completed(futures):
                cr_file = futures[future]
                
                # If shutdown requested, still collect the result but mark we should stop
                should_continue = not shutdown_flag.is_set()
                
                try:
                    result = future.result()
                    all_results.append(result)
                    
                    # Export result immediately
                    csv_exporter.export_test_results([result])
                    
                    # Export failed tests if any
                    if "failed_tests" in result and result["failed_tests"]:
                        csv_exporter.export_failed_tests(result["failed_tests"])
                    
                    # Export test execution times if any
                    if "test_execution_times" in result and result["test_execution_times"]:
                        csv_exporter.export_test_execution_times(result["test_execution_times"])
                    
                    # Log result
                    status = "PASSED" if result["passed"] else "FAILED"
                    logger.info(f"[Iteration {iteration}] {cr_file}: {status}")
                    
                except Exception as e:
                    logger.error(f"[Iteration {iteration}] Exception running {cr_file}: {e}")
                    all_results.append({
                        "cr_name": cr_file,
                        "passed": False,
                        "phase": "Exception",
                        "tests_passed": 0,
                        "tests_failed": 0,
                        "tests_skipped": 0,
                        "message": str(e),
                        "timestamp": datetime.now().isoformat(),
                        "iteration": iteration
                    })
                
                # Break after processing if shutdown was requested
                if not should_continue:
                    logger.info("Shutdown requested, waiting for remaining tasks to complete...")
                    break
        
        iteration += 1
        
        # Check if we should continue
        remaining_time = (end_time - datetime.now()).total_seconds()
        if remaining_time <= 0:
            logger.info("Time limit reached")
            break
        
        if shutdown_flag.is_set():
            logger.info("Shutdown requested")
            break
    
    logger.info(f"\nCompleted {iteration - 1} iterations")
    logger.info(f"Last iteration started at: {last_iteration_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    return all_results, last_iteration_start_time


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Run OpenStack Tempest tests with pod monitoring'
    )
    parser.add_argument(
        '-c', '--config',
        default='config.yaml',
        help='Path to configuration file (default: config.yaml)'
    )
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    # Setup logging
    setup_logging(
        config['logging']['level'],
        config['logging']['log_file']
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*60)
    logger.info("OpenStack Tempest Test Runner Started")
    logger.info("="*60)
    
    # Setup signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Initialize components
    pod_monitor = PodMonitor(
        namespace=config['monitoring']['namespace'],
        pod_patterns=config['monitoring']['pod_patterns'],
        interval=config['monitoring']['interval_seconds']
    )
    
    cr_handler = CRHandler(
        namespace=config['openshift']['cr_namespace'],
        timeout=config['openshift']['cr_timeout']
    )
    
    csv_exporter = CSVExporter(
        results_dir=config['output']['results_dir'],
        csv_filename=config['output']['csv_filename'],
        enable_graphs=config['output']['enable_graphs'],
        graph_format=config['output']['graph_format']
    )
    
    # Calculate start and end times
    test_start_time = datetime.now()
    end_time = test_start_time + timedelta(hours=config['time_to_run_hours'])
    logger.info(f"Test execution started at: {test_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Will run tests until: {end_time}")
    
    # Start pod monitoring in background thread
    monitor_thread = threading.Thread(
        target=monitor_pods_loop,
        args=(pod_monitor, csv_exporter, config['monitoring']['interval_seconds']),
        daemon=True
    )
    monitor_thread.start()
    
    # Run tests in loop
    last_iteration_start_time = test_start_time  # Default to test start time
    try:
        all_results, last_iteration_start_time = run_tests_in_loop(
            cr_files=config['cr_files'],
            cr_handler=cr_handler,
            csv_exporter=csv_exporter,
            end_time=end_time,
            max_parallel=len(config['cr_files'])  # Run all CRs in parallel
        )
        
        # Wait a bit for final metrics to be collected
        logger.info("Waiting for final metrics collection...")
        time.sleep(config['monitoring']['interval_seconds'] + 5)
        
    finally:
        # Signal monitoring thread to stop
        shutdown_flag.set()
        monitor_thread.join(timeout=10)
        
        # Clean up any remaining active CRs (using oc delete -f which cleans up pods too)
        if cr_handler.active_crs:
            logger.info("\nCleaning up active CRs...")
            for cr_name in list(cr_handler.active_crs.keys()):
                logger.info(f"Deleting active CR: {cr_name}")
                cr_handler.delete_cr(cr_name)
        
        # Analyze API performance
        api_data = {}
        api_graph_file = ""
        api_csv_file = ""
        
        # Detect which service is being tested from CR filenames
        service_filter = None
        if config['cr_files']:
            # Check CR filenames for service names
            cr_file_str = ' '.join(config['cr_files']).lower()
            if 'octavia' in cr_file_str:
                service_filter = 'octavia'
            elif 'designate' in cr_file_str:
                service_filter = 'designate'
            elif 'neutron' in cr_file_str:
                service_filter = 'neutron'
            elif 'nova' in cr_file_str:
                service_filter = 'nova'
            elif 'cinder' in cr_file_str:
                service_filter = 'cinder'
            elif 'glance' in cr_file_str:
                service_filter = 'glance'
        
        # Only analyze API logs if we detected a specific service
        if service_filter:
            logger.info("\nAnalyzing API pod logs...")
            logger.info(f"Detected test service from CR files: {service_filter}")
            logger.info(f"Filtering API logs from test start time: {test_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            try:
                # Use namespace from config, default to 'openstack'
                namespace = config.get('namespace', 'openstack')
                api_monitor = APIMonitor(namespace=namespace)
                api_data = api_monitor.analyze_all_api_pods(since_time=test_start_time, service_filter=service_filter)
                
                if api_data.get('total_requests', 0) > 0:
                    # Export API requests to CSV
                    api_csv_file = csv_exporter.export_api_requests(api_data)
                    
                    # Generate API performance graph
                    if config['output']['enable_graphs']:
                        api_graph_file = csv_exporter.generate_api_performance_graph(api_data)
                    
                    logger.info(f"API Analysis Summary:")
                    logger.info(f"  Total Requests: {api_data['total_requests']}")
                    logger.info(f"  Error Requests: {api_data['error_requests']}")
                    logger.info(f"  Success Rate: {api_data['success_rate']:.2f}%")
                    logger.info(f"  Avg Response Time: {api_data['avg_response_time']:.3f}s")
                    
                    # Show error endpoints if any
                    if api_data['error_requests'] > 0:
                        error_reqs = [r for r in api_data['requests'] if r['is_error']]
                        logger.info(f"\n  ðŸ”´ Error Endpoints (HTTP 4xx/5xx):")
                        
                        # Group errors by endpoint and status
                        error_summary = {}
                        for req in error_reqs:
                            key = (req['method'], req['endpoint'], req['status_code'])
                            if key not in error_summary:
                                error_summary[key] = {'count': 0, 'service': req['service']}
                            error_summary[key]['count'] += 1
                        
                        # Show up to 10 most common errors
                        sorted_errors = sorted(error_summary.items(), key=lambda x: x[1]['count'], reverse=True)[:10]
                        for (method, endpoint, status), info in sorted_errors:
                            logger.info(f"    [{info['service']}] {method} {endpoint} â†’ {status} ({info['count']}x)")
                else:
                    logger.warning("No API requests found in logs")
            except Exception as e:
                logger.error(f"Error analyzing API logs: {e}")
        else:
            logger.info("\nSkipping API analysis (could not detect service type from CR files)")
        
        # Collect error logs from OpenStack pods
        error_data = {}
        error_report_file = ""
        error_csv_file = ""
        logger.info("\nCollecting ERROR/CRITICAL logs from OpenStack pods...")
        logger.info("âš¡ Performance optimization: Analyzing LAST ITERATION ONLY (not entire test run)")
        logger.info(f"Filtering logs from last iteration start: {last_iteration_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"(Skipping earlier logs from: {test_start_time.strftime('%Y-%m-%d %H:%M:%S')} - {last_iteration_start_time.strftime('%Y-%m-%d %H:%M:%S')})")
        try:
            namespace = config.get('namespace', 'openstack')
            error_collector = ErrorCollector(namespace=namespace)
            error_data = error_collector.collect_all_errors(since_time=last_iteration_start_time, service_filter=service_filter)
            
            if error_data.get('total_errors', 0) > 0:
                # Export error log to CSV
                error_csv_file = csv_exporter.export_error_log(error_data)
                
                # Generate error report HTML
                error_report_file = csv_exporter.generate_error_report(error_data)
                
                logger.info(f"Error Collection Summary:")
                logger.info(f"  Total Errors: {error_data['total_errors']}")
                logger.info(f"  Unique Errors: {error_data['unique_count']}")
                logger.info(f"  Critical Errors: {error_data['critical_count']}")
                logger.info(f"  Pods Analyzed: {len(error_data['pods_analyzed'])}")
                
                # Show error breakdown by service
                if error_data.get('by_service'):
                    logger.info(f"\n  ðŸ“Š Errors by Service:")
                    for service, count in sorted(error_data['by_service'].items(), key=lambda x: x[1], reverse=True):
                        logger.info(f"    {service}: {count}")
            else:
                logger.info("âœ… No ERROR/CRITICAL logs found in OpenStack pods during test execution")
        except Exception as e:
            logger.error(f"Error collecting error logs: {e}")
        
        # Generate graphs and track for web report
        graph_files = []
        
        if config['output']['enable_graphs']:
            logger.info("Generating graphs...")
            try:
                graph_files = csv_exporter.generate_graphs()
                
                # Add API graph if available
                if api_graph_file:
                    graph_files.append(api_graph_file)
                
                # Add error report if available
                if error_report_file:
                    graph_files.append(error_report_file)
                
                if graph_files:
                    for graph_file in graph_files:
                        logger.info(f"Generated graph: {graph_file}")
                else:
                    logger.warning("No graphs were generated (possibly not enough data)")
            except Exception as e:
                logger.error(f"Failed to generate graphs: {e}")
        
        # Generate web report
        logger.info("Generating web report...")
        try:
            # Calculate test-level statistics (not CR-level)
            total_tests_passed = sum(r.get('tests_passed', 0) for r in all_results)
            total_tests_failed = sum(r.get('tests_failed', 0) for r in all_results)
            total_tests_skipped = sum(r.get('tests_skipped', 0) for r in all_results)
            total_tests = total_tests_passed + total_tests_failed + total_tests_skipped
            
            test_summary = {
                'total_runs': len(all_results),  # Number of CR executions
                'total_tests': total_tests,  # Total individual tests
                'tests_passed': total_tests_passed,
                'tests_failed': total_tests_failed,
                'tests_skipped': total_tests_skipped
            }
            web_report_dir = csv_exporter.generate_web_report(test_summary, graph_files)
            if web_report_dir:
                logger.info(f"Web report generated at: {web_report_dir}")
        except Exception as e:
            logger.error(f"Failed to generate web report: {e}")
        
        # Create zip archive of all results
        logger.info("Creating results archive...")
        archive_file = csv_exporter.create_results_archive()
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("TEST RUN SUMMARY")
        logger.info("="*60)
        logger.info(f"Total test runs: {len(all_results)}")
        
        if all_results:
            passed = sum(1 for r in all_results if r['passed'])
            failed = sum(1 for r in all_results if not r['passed'])
            logger.info(f"Passed: {passed}")
            logger.info(f"Failed: {failed}")
            logger.info(f"Success rate: {passed/len(all_results)*100:.1f}%")
        
        logger.info(f"\nMetrics CSV: {csv_exporter.metrics_csv}")
        logger.info(f"Results CSV: {csv_exporter.results_csv}")
        if os.path.exists(csv_exporter.failed_tests_csv):
            logger.info(f"Failed Tests CSV: {csv_exporter.failed_tests_csv}")
        if os.path.exists(csv_exporter.test_execution_csv):
            logger.info(f"Test Execution Times CSV: {csv_exporter.test_execution_csv}")
        if api_csv_file and os.path.exists(api_csv_file):
            logger.info(f"API Requests CSV: {api_csv_file}")
        
        if error_csv_file and os.path.exists(error_csv_file):
            logger.info(f"Error Log CSV: {error_csv_file}")
        
        # Show web report location
        if 'web_report_dir' in locals() and web_report_dir:
            logger.info(f"\nðŸ“Š Web Report: {web_report_dir}/index.html")
            logger.info(f"    Upload the 'web_report' directory to your HTTP server")
        
        # Print download command for the zip archive
        if archive_file and os.path.exists(archive_file):
            # ANSI color codes
            GREEN = "\033[1;32m"
            CYAN = "\033[1;36m"
            YELLOW = "\033[1;33m"
            BLUE = "\033[1;34m"
            RESET = "\033[0m"
            
            print(f"\n{CYAN}{'='*60}{RESET}")
            print(f"{GREEN}DOWNLOAD COMMAND FOR RESULTS ARCHIVE{RESET}")
            print(f"{CYAN}{'='*60}{RESET}")
            print(f"{YELLOW}All results are packaged in a single ZIP file.{RESET}")
            print(f"Copy and paste this command on your local desktop:")
            print(f"(Replace <your_bastion_host> with your actual bastion hostname)\n")
            
            filename = os.path.basename(archive_file)
            full_path = os.path.abspath(archive_file)
            
            print(f"{BLUE}# Download all results (ZIP archive):{RESET}")
            # Use base64 to safely transfer binary file through SSH (prevents corruption)
            # Note: No -t flag - it corrupts base64 data stream
            download_cmd = f'ssh root@<your_bastion_host> "su - zuul -c \'ssh -q controller-0 \\\"base64 {full_path}\\\"\'" | base64 -d > {filename}'
            print(f"{GREEN}{download_cmd}{RESET}")
            print(f"{YELLOW}Note: Using base64 encoding to safely transfer binary ZIP file{RESET}\n")
            
            print(f"Archive contains: CSV files, HTML graphs, and PNG images")
            print(f"Archive size: {os.path.getsize(archive_file) / 1024 / 1024:.2f} MB")
            print(f"Archive location: {full_path}")
            print(f"{CYAN}{'='*60}{RESET}")
        
        logger.info("="*60)
        logger.info("OpenStack Tempest Test Runner Completed")
        logger.info("="*60)


if __name__ == "__main__":
    main()

