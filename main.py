#!/usr/bin/env python3
"""
Generated by Cursor AI Assistant
https://cursor.sh

OpenStack Tempest Test Runner with Pod Monitoring

This tool runs OpenStack Tempest tests via OpenShift Custom Resources
in parallel and monitors pod metrics (CPU, RAM, status) throughout the test runs.
"""

import argparse
import logging
import os
import signal
import socket
import sys
import threading
import time
import yaml
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple

from pod_monitor import PodMonitor
from cr_handler import CRHandler
from csv_exporter import CSVExporter


# Global flag for graceful shutdown
shutdown_flag = threading.Event()


def setup_logging(log_level: str, log_file: str):
    """Setup logging configuration."""
    numeric_level = getattr(logging, log_level.upper(), logging.INFO)
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(numeric_level)
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(console_formatter)
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(numeric_level)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)


def load_config(config_file: str) -> Dict:
    """Load configuration from YAML file."""
    try:
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        logging.error(f"Configuration file not found: {config_file}")
        sys.exit(1)
    except yaml.YAMLError as e:
        logging.error(f"Error parsing configuration file: {e}")
        sys.exit(1)


def signal_handler(signum, frame):
    """Handle interrupt signals for graceful shutdown."""
    logging.info("Received interrupt signal. Initiating graceful shutdown...")
    shutdown_flag.set()


def print_error_bold_red(message: str):
    """Print error message in bold red text."""
    # ANSI color codes: \033[1;31m = bold red, \033[0m = reset
    RED_BOLD = "\033[1;31m"
    RESET = "\033[0m"
    print(f"{RED_BOLD}{message}{RESET}")
    logging.error(message)


def run_cr_with_monitoring(cr_file: str, cr_handler: CRHandler, iteration: int) -> Dict:
    """
    Run a single CR and monitor until completion.
    
    Args:
        cr_file: Path to CR file
        cr_handler: CRHandler instance
        iteration: Current iteration number
        
    Returns:
        Dictionary with test results
    """
    logger = logging.getLogger(__name__)
    
    # Apply the CR
    success, cr_name, error = cr_handler.apply_cr(cr_file)
    
    if not success:
        logger.error(f"Failed to apply CR {cr_file}: {error}")
        return {
            "cr_name": cr_file,
            "passed": False,
            "phase": "ApplyFailed",
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "message": error,
            "timestamp": datetime.now().isoformat(),
            "iteration": iteration
        }
    
    logger.info(f"[Iteration {iteration}] Started CR: {cr_name} from {cr_file}")
    
    # Wait for pod to actually start (fail fast if pod doesn't start)
    pod_started, pod_message = cr_handler.wait_for_pod_to_start(
        cr_name, 
        timeout=180,  # 3 minutes to start
        shutdown_flag=shutdown_flag
    )
    
    if not pod_started:
        error_msg = f"ERROR: Test pod failed to start for CR {cr_name}!"
        detail_msg = f"Details: {pod_message}"
        explanation = "This indicates a problem with the test setup or environment."
        action = "Stopping tool execution immediately."
        
        print_error_bold_red("\n" + "="*60)
        print_error_bold_red(error_msg)
        print_error_bold_red(detail_msg)
        print_error_bold_red(explanation)
        print_error_bold_red(action)
        print_error_bold_red("="*60 + "\n")
        
        # Clean up the failed CR
        logger.info(f"Cleaning up failed CR: {cr_name}")
        cr_handler.delete_cr(cr_name)
        
        # Signal shutdown to stop the tool
        shutdown_flag.set()
        
        return {
            "cr_name": cr_file,
            "passed": False,
            "phase": "PodStartupFailed",
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "message": f"Pod failed to start: {pod_message}",
            "timestamp": datetime.now().isoformat(),
            "iteration": iteration
        }
    
    # Wait for completion (non-blocking, returns after timeout or completion)
    # Pass shutdown_flag to allow graceful interruption
    success, message = cr_handler.wait_for_completion(cr_name, poll_interval=30, shutdown_flag=shutdown_flag)
    
    # Check results
    results = cr_handler.check_test_results(cr_name)
    results["iteration"] = iteration
    
    if not success:
        logger.error(f"[Iteration {iteration}] CR {cr_name} failed: {message}")
    else:
        logger.info(f"[Iteration {iteration}] CR {cr_name} completed successfully")
    
    # Extract failed tests from pod logs before cleanup
    logger.info(f"[Iteration {iteration}] Extracting failed tests for CR: {cr_name}")
    failed_tests = cr_handler.extract_failed_tests(cr_name)
    
    # Store failed tests in results for export
    results["failed_tests"] = failed_tests
    
    # Clean up CR after completion (needed for next iteration)
    # Test-operator leaves pods in ERROR status when complete
    logger.info(f"[Iteration {iteration}] Cleaning up CR: {cr_name}")
    if cr_handler.delete_cr(cr_name):
        logger.info(f"[Iteration {iteration}] Successfully deleted CR: {cr_name}")
    else:
        logger.warning(f"[Iteration {iteration}] Failed to delete CR: {cr_name}, may need manual cleanup")
    
    return results


def monitor_pods_loop(pod_monitor: PodMonitor, csv_exporter: CSVExporter, interval: int):
    """
    Continuously monitor pods in a separate thread.
    
    Args:
        pod_monitor: PodMonitor instance
        csv_exporter: CSVExporter instance
        interval: Monitoring interval in seconds
    """
    logger = logging.getLogger(__name__)
    logger.info("Started pod monitoring loop")
    
    while not shutdown_flag.is_set():
        try:
            # Collect metrics
            metrics = pod_monitor.collect_metrics()
            
            if metrics:
                logger.debug(f"Collected metrics for {len(metrics)} pods")
                # Export to CSV
                csv_exporter.export_metrics(metrics)
            
            # Wait for next interval
            shutdown_flag.wait(timeout=interval)
            
        except Exception as e:
            logger.error(f"Error in monitoring loop: {e}")
            time.sleep(interval)
    
    logger.info("Pod monitoring loop stopped")


def run_tests_in_loop(cr_files: List[str], cr_handler: CRHandler, csv_exporter: CSVExporter, 
                      end_time: datetime, max_parallel: int = 2) -> List[Dict]:
    """
    Run CR tests in a loop until end_time.
    
    Args:
        cr_files: List of CR files to run
        cr_handler: CRHandler instance
        csv_exporter: CSVExporter instance
        end_time: Time to stop running tests
        max_parallel: Maximum parallel CR executions
        
    Returns:
        List of all test results
    """
    logger = logging.getLogger(__name__)
    all_results = []
    iteration = 1
    
    logger.info(f"Starting test loop with {len(cr_files)} CR files")
    logger.info(f"Will run until: {end_time}")
    
    while datetime.now() < end_time and not shutdown_flag.is_set():
        logger.info(f"\n{'='*60}")
        logger.info(f"Starting iteration {iteration}")
        logger.info(f"{'='*60}\n")
        
        # Run CRs in parallel
        with ThreadPoolExecutor(max_workers=max_parallel) as executor:
            futures = {}
            
            for cr_file in cr_files:
                if shutdown_flag.is_set():
                    break
                    
                future = executor.submit(run_cr_with_monitoring, cr_file, cr_handler, iteration)
                futures[future] = cr_file
            
            # Collect results as they complete
            for future in as_completed(futures):
                cr_file = futures[future]
                
                # If shutdown requested, still collect the result but mark we should stop
                should_continue = not shutdown_flag.is_set()
                
                try:
                    result = future.result()
                    all_results.append(result)
                    
                    # Export result immediately
                    csv_exporter.export_test_results([result])
                    
                    # Export failed tests if any
                    if "failed_tests" in result and result["failed_tests"]:
                        csv_exporter.export_failed_tests(result["failed_tests"])
                    
                    # Log result
                    status = "PASSED" if result["passed"] else "FAILED"
                    logger.info(f"[Iteration {iteration}] {cr_file}: {status}")
                    
                except Exception as e:
                    logger.error(f"[Iteration {iteration}] Exception running {cr_file}: {e}")
                    all_results.append({
                        "cr_name": cr_file,
                        "passed": False,
                        "phase": "Exception",
                        "tests_passed": 0,
                        "tests_failed": 0,
                        "tests_skipped": 0,
                        "message": str(e),
                        "timestamp": datetime.now().isoformat(),
                        "iteration": iteration
                    })
                
                # Break after processing if shutdown was requested
                if not should_continue:
                    logger.info("Shutdown requested, waiting for remaining tasks to complete...")
                    break
        
        iteration += 1
        
        # Check if we should continue
        remaining_time = (end_time - datetime.now()).total_seconds()
        if remaining_time <= 0:
            logger.info("Time limit reached")
            break
        
        if shutdown_flag.is_set():
            logger.info("Shutdown requested")
            break
    
    logger.info(f"\nCompleted {iteration - 1} iterations")
    return all_results


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Run OpenStack Tempest tests with pod monitoring'
    )
    parser.add_argument(
        '-c', '--config',
        default='config.yaml',
        help='Path to configuration file (default: config.yaml)'
    )
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    # Setup logging
    setup_logging(
        config['logging']['level'],
        config['logging']['log_file']
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*60)
    logger.info("OpenStack Tempest Test Runner Started")
    logger.info("="*60)
    
    # Setup signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Initialize components
    pod_monitor = PodMonitor(
        namespace=config['monitoring']['namespace'],
        pod_patterns=config['monitoring']['pod_patterns'],
        interval=config['monitoring']['interval_seconds']
    )
    
    cr_handler = CRHandler(
        namespace=config['openshift']['cr_namespace'],
        timeout=config['openshift']['cr_timeout']
    )
    
    csv_exporter = CSVExporter(
        results_dir=config['output']['results_dir'],
        csv_filename=config['output']['csv_filename'],
        enable_graphs=config['output']['enable_graphs'],
        graph_format=config['output']['graph_format']
    )
    
    # Calculate end time
    end_time = datetime.now() + timedelta(hours=config['time_to_run_hours'])
    logger.info(f"Will run tests until: {end_time}")
    
    # Start pod monitoring in background thread
    monitor_thread = threading.Thread(
        target=monitor_pods_loop,
        args=(pod_monitor, csv_exporter, config['monitoring']['interval_seconds']),
        daemon=True
    )
    monitor_thread.start()
    
    # Run tests in loop
    try:
        all_results = run_tests_in_loop(
            cr_files=config['cr_files'],
            cr_handler=cr_handler,
            csv_exporter=csv_exporter,
            end_time=end_time,
            max_parallel=len(config['cr_files'])  # Run all CRs in parallel
        )
        
        # Wait a bit for final metrics to be collected
        logger.info("Waiting for final metrics collection...")
        time.sleep(config['monitoring']['interval_seconds'] + 5)
        
    finally:
        # Signal monitoring thread to stop
        shutdown_flag.set()
        monitor_thread.join(timeout=10)
        
        # Clean up any remaining active CRs (using oc delete -f which cleans up pods too)
        if cr_handler.active_crs:
            logger.info("\nCleaning up active CRs...")
            for cr_name in list(cr_handler.active_crs.keys()):
                logger.info(f"Deleting active CR: {cr_name}")
                cr_handler.delete_cr(cr_name)
        
        # Generate graphs
        graph_files = []
        if config['output']['enable_graphs']:
            logger.info("Generating graphs...")
            try:
                graph_files = csv_exporter.generate_graphs()
                if graph_files:
                    for graph_file in graph_files:
                        logger.info(f"Generated graph: {graph_file}")
                else:
                    logger.warning("No graphs were generated (possibly not enough data)")
            except Exception as e:
                logger.error(f"Failed to generate graphs: {e}")
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("TEST RUN SUMMARY")
        logger.info("="*60)
        logger.info(f"Total test runs: {len(all_results)}")
        
        if all_results:
            passed = sum(1 for r in all_results if r['passed'])
            failed = sum(1 for r in all_results if not r['passed'])
            logger.info(f"Passed: {passed}")
            logger.info(f"Failed: {failed}")
            logger.info(f"Success rate: {passed/len(all_results)*100:.1f}%")
        
        logger.info(f"\nMetrics CSV: {csv_exporter.metrics_csv}")
        logger.info(f"Results CSV: {csv_exporter.results_csv}")
        if os.path.exists(csv_exporter.failed_tests_csv):
            logger.info(f"Failed Tests CSV: {csv_exporter.failed_tests_csv}")
        
        # Print download commands for result files (example commands)
        current_host = socket.gethostname()
        current_dir = os.getcwd()
        
        # Collect available files
        csv_files = []
        html_files = []
        
        if os.path.exists(csv_exporter.metrics_csv):
            csv_files.append(csv_exporter.metrics_csv)
        if os.path.exists(csv_exporter.results_csv):
            csv_files.append(csv_exporter.results_csv)
        if os.path.exists(csv_exporter.failed_tests_csv):
            csv_files.append(csv_exporter.failed_tests_csv)
        
        # Collect HTML files from graph_files list
        for graph_file in graph_files:
            if graph_file and graph_file.endswith('.html'):
                html_files.append(graph_file)
        
        # Also check for any existing HTML files in results directory
        if os.path.exists(config['output']['results_dir']):
            for filename in os.listdir(config['output']['results_dir']):
                if filename.endswith('.html'):
                    html_path = os.path.join(config['output']['results_dir'], filename)
                    if html_path not in html_files:  # Avoid duplicates
                        html_files.append(html_path)
        
        if csv_files or html_files:
            logger.info("\n" + "="*60)
            logger.info("DOWNLOAD COMMANDS FOR RESULT FILES")
            logger.info("="*60)
            logger.info("Copy and paste these commands on your local desktop:")
            logger.info("(Replace <your_bastion_host> with your actual bastion hostname)\n")
            
            # Show CSV download command (one example)
            if csv_files:
                csv_file = csv_files[0]  # Use first CSV as example
                filename = os.path.basename(csv_file)
                full_path = os.path.abspath(csv_file)
                logger.info("# Download CSV files (example):")
                download_cmd = f'ssh -t root@<your_bastion_host> "su - zuul -c \'ssh -q controller-0 \"cat {full_path}\"\'" > {filename}'
                logger.info(f"{download_cmd}\n")
            
            # Show HTML graph download commands (all HTML files)
            if html_files:
                logger.info("# Download HTML graphs:")
                for html_file in html_files:
                    filename = os.path.basename(html_file)
                    full_path = os.path.abspath(html_file)
                    download_cmd = f'ssh -t root@<your_bastion_host> "su - zuul -c \'ssh -q controller-0 \"cat {full_path}\"\'" > {filename}'
                    logger.info(f"{download_cmd}")
                logger.info("")
            
            # List all available files
            logger.info("All result files:")
            for csv_file in csv_files:
                logger.info(f"  - {os.path.abspath(csv_file)}")
            for html_file in html_files:
                logger.info(f"  - {os.path.abspath(html_file)}")
        
        logger.info("="*60)
        logger.info("OpenStack Tempest Test Runner Completed")
        logger.info("="*60)


if __name__ == "__main__":
    main()

